spring:
  application:
    name: ASpider
  profiles:
    active: dev
elasticsearch: # Elasticsearch配置
  host: xxx
  port: xxx
logging: # 日志配置
  level:
    root: INFO # 全局日志级别
#    com.ershi.aspider: DEBUG # 开启可用于详细调试
  pattern:
    console: "%d{yyyy-MM-dd HH:mm:ss} [%thread] %-5level %logger{36} - %msg%n"

embedding: # 文本向量化配置
  provider: qwen # 选择向量化提供商：zhipu（智谱）、qwen（千问）、openai（OpenAI兼容）
  max-batch-size: 10        # 每次API调用最大文本条数
  rpm-limit: 60             # 每分钟最大请求数（RPM）
  qwen:
    api-key: your-qwen-api-key
    model: text-embedding-v4
  zhipu:
    api-key: your-zhipu-api-key
    model: embedding-3  # 向量化模型名称
  openai:
    base-url: https://api.openai.com/v1  # 支持自定义端点（如第三方代理）
    api-key: your-openai-api-key
    model: Qwen3-Embedding-8B        # 模型名称                 # 向量维度
processor: # 数据处理模块
  scorer: # 文章评分配置
    strategy: rule  # 评分策略：rule（规则）/ llm（大模型）
  summary-quality: # 摘要质量评估配置
    enable: false                      # 是否启用摘要质量评估
    low-quality-threshold: 60          # 低质量阈值（低于此分数强制触发LLM）
    high-quality-threshold: 75         # 高质量阈值（高于此分数无需处理）
    min-length: 30                     # 摘要最小有效长度
    ideal-length-min: 80               # 摘要理想最小长度
    ideal-length-max: 200              # 摘要理想最大长度
    max-length: 400                    # 摘要最大长度
    title-similarity-threshold: 0.7    # 标题相似度阈值
    force-high-value: false            # 是否对高价值文章强制触发LLM
    max-llm-per-batch: 40              # 单批次LLM调用上限
  content-extraction: # 文章数据摘要提取配置，用于提取长文本内容，输出摘要供后续操作
    short-text-threshold: 500      # 短文本阈值
    medium-text-threshold: 2000    # 中文本阈值
    medium-text-truncate-length: 600  # 中文本截取长度
    llm-summary: # LLM摘要提取配置
      enable: false                # 是否启用LLM总结
      target-length: 200           # 摘要目标长度
      provider: qwen               # 提供商：qwen/gemini
      model: qwen3-max            # 模型名称
      api-key: your-llm-api-key    # API密钥
      base-url:                    # 自定义请求api，Gemini支持使用第三方api，未填写则使用官方api

analysis: # 分析模块配置
  summary-fallback: # 分析阶段摘要兜底配置
    enable: false                      # 是否启用分析阶段兜底
    min-quality-score: 60              # 触发兜底的最低质量评分
    topk-limit: 10                     # 兜底处理的TopK限制
    max-llm-per-request: 5             # 单次分析请求最大LLM调用数
    write-back: false                  # 是否写回ES
    cache:
      type: memory                     # 缓存类型
      max-size: 2000                   # 最大缓存条数
      ttl: 12h                         # 缓存过期时间
  agent: # Agent分析模块
    llm: # LLM多API配置（支持全局默认 + 按Agent覆盖）
      default: # 全局默认配置
        base-url: ${OPENAI_BASE_URL:https://api.openai.com/v1}
        api-key: ${OPENAI_API_KEY:your-api-key}
        model: ${OPENAI_MODEL:gpt-4o-mini}
        timeout: 30                   # 超时秒数
        max-retries: 3                # 最大重试次数
      # PolicyAgent专用配置（可选覆盖）
      # policy:
      #   model: gpt-4o-mini          # 可使用更轻量模型
      # SectorAgent专用配置（可选覆盖）
      # sector:
      # TrendAgent专用配置（可选覆盖）
      # trend:
      # SynthesisAgent专用配置（推荐使用更强模型）
      synthesis:
        model: ${SYNTHESIS_MODEL:gpt-4o}  # 综合研判建议使用更强模型
